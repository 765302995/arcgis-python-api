{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Administering and Securing Organizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Patterns and Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arcgis\n",
    "from arcgis.gis import GIS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from arcgis._impl.common._utils import chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis = GIS(profile='your_online_profile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Event Feature Layer as SeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = gis.content.search(\"Threat Detection Data\", item_type=\"Feature Layer\")[0]\n",
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Acquire the Last Date in SeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = True\n",
    "lyrs = item.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sdf_storage = lyrs.query(as_df=True)\n",
    "except:\n",
    "    sdf_storage = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "end_date_orig = datetime.datetime.now()\n",
    "end_date = copy.deepcopy(end_date_orig)\n",
    "try:\n",
    "    last_date = sdf_storage.created.max()\n",
    "    start_date_orig = copy.deepcopy(last_date.to_pydatetime())\n",
    "    start_date = last_date.to_pydatetime()\n",
    "except:\n",
    "    start_date_orig = end_date_orig - datetime.timedelta(days=200)\n",
    "    start_date= end_date_orig - datetime.timedelta(days=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download CSV Data for Each Day "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Downloads the data for each day not in the Feature Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "i = 1\n",
    "days = (end_date_orig - start_date_orig).days\n",
    "print(days)\n",
    "if days == 0:\n",
    "    days = 1\n",
    "end_date = copy.deepcopy(end_date_orig)\n",
    "while i <= days:\n",
    "    start_date = end_date_orig - datetime.timedelta(days=i)\n",
    "    \n",
    "    fp = gis.admin.history(start_date, to_date=end_date, all_events=True, data_format='csv')\n",
    "    end_date = start_date\n",
    "    data.append(\n",
    "        pd.read_csv(fp, infer_datetime_format=True, parse_dates=True, dtype={'ip' : str}))\n",
    "    os.remove(fp)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge and Clean up Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform simple data cleanups to ensure data is readable and useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data) == 1:\n",
    "    data = data[0]\n",
    "else:\n",
    "    data = pd.concat(data)\n",
    "data.created = pd.to_datetime(data.created, unit='ms')\n",
    "data.columns = [c.lower() if c not in ['SHAPE', 'OBJECTID'] \\\n",
    "                else c for c in data.columns]\n",
    "criteria = [data['created'].dt.hour.between(0,1), # \"LATE NIGHT\"\n",
    "            data['created'].dt.hour.between(2,5), # \"OVERNIGHT\"\n",
    "            data['created'].dt.hour.between(6,9), # \"MORNING\"\n",
    "            data['created'].dt.hour.between(10,13), # \"MID-DAY\"\n",
    "            data['created'].dt.hour.between(14,17), # \"AFTERNOON\"\n",
    "            data['created'].dt.hour.between(18,21), # \"EVENING\"\n",
    "            data['created'].dt.hour.between(22,24)] # \"LATE NIGHT\"\n",
    "blocks = [\"LATE NIGHT\", \"OVERNIGHT\", \"MORNING\", \"MID-DAY\", \n",
    "          \"AFTERNOON\", \"EVENING\", \"LATE NIGHT\"]\n",
    "data['time_block'] = np.select(criteria, blocks, 0)\n",
    "data['day_of_week'] = data.created.dt.weekday_name\n",
    "# Backup Data\n",
    "path = r\"./data\"\n",
    "backup_file = datetime.datetime.now().strftime('%Y%m%d_%H_%M_history.csv')\n",
    "data.to_csv(f\"{path}/{backup_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = start_date_orig\n",
    "df_ips_only = data[data.ip.str.strip().str.len() > 0].copy().reset_index(drop=True)\n",
    "cols_show = [c for c in df_ips_only.columns if c !='ip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ips_only[cols_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoLocating IP Addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding IPs\n",
    "\n",
    "- leverage `geoip2` python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoip2\n",
    "from geoip2 import database\n",
    "reader = database.Reader(r\"./spatial_data/GeoLite2-City.mmdb\")\n",
    "\n",
    "def geocode_ip(ips, reader):\n",
    "    \"\"\"Builds a reader using geoip2 and the GeoList2-City db to locate IPs\"\"\"\n",
    "    for ip in ips:\n",
    "        try:\n",
    "            res = reader.city(ip)\n",
    "        \n",
    "            yield (ip, res.continent.name, res.country.name, \n",
    "                   res.country.iso_code, res.location.latitude,\n",
    "                   res.location.longitude, res.registered_country.iso_code, \n",
    "                   res.postal.code)\n",
    "        except:\n",
    "            yield (ip, None, None, None, None, None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips = df_ips_only.ip.unique()\n",
    "columns = ['ip', 'continent', \"country\", \"isocode\", \"lat\", \"long\", \"reg_cntry\", \"postal\"]\n",
    "records = [rec for rec in geocode_ip(ips, reader)]\n",
    "df_ips = pd.DataFrame(records, columns=columns)\n",
    "df_ips[df_ips.columns.tolist()[1:]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoSpatial Distribution of IP Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.DataFrame.spatial.from_xy(df_ips, x_column=\"long\", y_column=\"lat\")\n",
    "sdf.spatial.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Massage the data into a format that can be exported to a fgdb and uploaded to the portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the Records to the GeoLocations \n",
    "\n",
    "Use `pd.merge` to combine the two dataframes into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ips.ip = df_ips.ip.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_j = pd.merge(df_ips_only,\n",
    "                sdf,\n",
    "                left_on='ip',\n",
    "                right_on='ip',\n",
    "                how='left').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Clean up field names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_j['OBJECTID'] = np.arange(1, len(df_j)+ 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_j.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine Event Time Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_j['time_block'].value_counts().plot('bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Examine the Spatial Distribution of the Portal's Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.4)\n",
    "(df_j['country']\n",
    " .value_counts()\n",
    " .plot(\"bar\", cmap='viridis', alpha=0.75, rot=45))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Outliers\n",
    "\n",
    "The goal is to find potential odd or uncommon locations that do not reflect common patterns within our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sdf_storage is None:\n",
    "    sdf_merged = pd.concat([sdf_storage, df_j], sort=False)\n",
    "else:\n",
    "    sdf_storage = pd.DataFrame([], columns=df_j.columns)\n",
    "    sdf_merged = df_j.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_make = LabelEncoder()\n",
    "sdf_merged['encoded_action'] = lb_make.fit_transform(sdf_merged.action)\n",
    "sdf_merged['encoded_day_of_week'] = lb_make.fit_transform(sdf_merged.day_of_week)\n",
    "sdf_merged['encoded_country'] = lb_make.fit_transform(np.where(sdf_merged.country.isnull(), \n",
    "                                                               \"\", \n",
    "                                                               sdf_merged.country.values))\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "data = sdf_merged[['lat', 'long',  'encoded_day_of_week', 'encoded_country']].values#'encoded_action',\n",
    "pca.fit(data)\n",
    "reduce_n2 = pca.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use Isolation Forests\n",
    "\n",
    "Isolation Forest is an outlier detection technique that identifies anomalies instead of normal observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(reduce_n2)\n",
    "clf = IsolationForest(max_samples='auto',\n",
    "                      behaviour=\"new\", \n",
    "                      contamination=.01)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_outliers = clf.predict(reduce_n2)\n",
    "sdf_merged['outliers_new'] = y_pred_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_merged.outliers_new.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Feature Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_updates = ~(sdf_merged[:len(sdf_storage)].copy().outliers.fillna(0).astype(int) == \\\n",
    "              sdf_merged[:len(sdf_storage)].copy().outliers_new.astype(int))\n",
    "sdf_merged['outliers'] = sdf_merged.outliers_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_merged = sdf_merged.drop(columns=['outliers_new', 'encoded_day_of_week', 'encoded_action'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data to Feature Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sdf_storage is None and len(sdf_storage) > 0:\n",
    "    fs_updates = [f for f in sdf_merged[:len(sdf_storage)][q_updates].copy().spatial.to_featureset()]\n",
    "    fs_new = [f for f in sdf_merged[len(sdf_storage):].copy().spatial.to_featureset()]\n",
    "    for idx, f in enumerate(fs_new):\n",
    "        f.as_dict['attributes'].pop(\"OBJECTID\")\n",
    "        fs_new[idx] = f\n",
    "else:\n",
    "    fs_updates = []\n",
    "    fs_new = [f.as_dict for f in sdf_merged.copy().spatial.to_featureset()]\n",
    "    for idx, f in enumerate(fs_new):\n",
    "        f['attributes'].pop(\"OBJECTID\")\n",
    "        fs_new[idx] = f    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Updates to Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = {}\n",
    "i = 0\n",
    "for chunk in chunks(fs_updates, 500):\n",
    "    response = lyrs.edit_features(updates=chunk)\n",
    "    resp[i] = response\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add New Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_add = {}\n",
    "i = 0\n",
    "for chunk in chunks(fs_new, 500):\n",
    "    response = lyrs.edit_features(adds=fs_new)\n",
    "    resp_add[i] = response\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
